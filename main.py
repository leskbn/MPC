# %%
import gymnasium as gym
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt


# %%ëª¨ë¸ ì •ì˜
class MLPRegressor(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, output_dim),
        )

    def forward(self, x):
        return self.net(x)


# %% ë°ì´í„° ìˆ˜ì§‘
print("[1] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
env = gym.make("CartPole-v1")
X, Y = [], []

for ep in range(200):
    state, _ = env.reset()
    done = False
    step = 0
    while not done:
        action = env.action_space.sample()
        next_state, _, done, _, _ = env.step(action)

        X.append(np.append(state, action))
        Y.append([next_state[2]])

        if ep % 50 == 0 and step == 0:
            print(
                f"  ì˜ˆì‹œ ìƒ˜í”Œ: state={state}, action={action}, next_angle={next_state[2]:.4f}"
            )

        state = next_state
        step += 1
print(f"ì´ ìˆ˜ì§‘ ìƒ˜í”Œ ìˆ˜: {len(X)}")

X = np.array(X, dtype=np.float32)
Y = np.array(Y, dtype=np.float32)

# %%ëª¨ë¸ í•™ìŠµ
print("[2] ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
model = MLPRegressor(input_dim=5, output_dim=1)
optimizer = optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.MSELoss()

for epoch in range(1, 101):
    model.train()
    inputs = torch.from_numpy(X)
    targets = torch.from_numpy(Y)

    preds = model(inputs)
    loss = loss_fn(preds, targets)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if epoch % 20 == 0 or epoch == 1:
        print(f"  Epoch {epoch:3d} | Loss: {loss.item():.6f}")


# %% MPC ì•¡ì…˜ ì„ íƒ í•¨ìˆ˜
def mpc_select_action(state, model, horizon=3):
    best_action = 0
    best_score = float("-inf")

    print(f"\n[MPC] í˜„ì¬ ìƒíƒœ: angle={state[2]:.4f}")
    for seq in range(2**horizon):
        actions = [(seq >> i) & 1 for i in range(horizon)]
        sim_state = state.copy()
        total_reward = 0

        print(f"  ì‹œí€€ìŠ¤ {actions} â†’ ì˜ˆì¸¡ angle:", end=" ")

        for a in actions:
            input_tensor = torch.tensor(
                np.append(sim_state, a), dtype=torch.float32
            ).unsqueeze(0)
            pred_angle = model(input_tensor).item()
            total_reward += -abs(pred_angle)
            sim_state[2] = pred_angle

            print(f"{pred_angle:.4f}", end=" â†’ ")

        print(f"í•©ê³„ ë³´ìƒ: {total_reward:.4f}")

        if total_reward > best_score:
            best_score = total_reward
            best_action = actions[0]

    print(f"ğŸ‘‰ ì„ íƒëœ ì•¡ì…˜: {best_action} (ë³´ìƒ: {best_score:.4f})")
    return best_action


# %% ì œì–´ ì‹¤í–‰
print("\n[3] MPC ì œì–´ ì‹œì‘...")

NUM_EPISODES = 5  # ì›í•˜ëŠ” ì—í”¼ì†Œë“œ ìˆ˜
MAX_STEPS = 200  # í•œ ì—í”¼ì†Œë“œì—ì„œ ìµœëŒ€ ìŠ¤í… ìˆ˜
env = gym.make("CartPole-v1", render_mode="human")

for ep in range(1, NUM_EPISODES + 1):
    print(f"\n=== Episode {ep} ì‹œì‘ ===")
    state, _ = env.reset()
    for t in range(1, MAX_STEPS + 1):
        env.render()

        action = mpc_select_action(state, model)
        next_state, _, done, _, _ = env.step(action)

        print(
            f"[Ep {ep:2d} | Step {t:3d}] angle: {state[2]: .4f} â†’ action: {action} â†’ next_angle: {next_state[2]: .4f}"
        )

        if done:
            print(f"  â†’ Episode {ep} ì¢…ë£Œ: ë§‰ëŒ€ê°€ {t} ìŠ¤í…ë§Œì— ë„˜ì–´ì§!")
            break

        state = next_state
    else:
        print(f"  â†’ Episode {ep} ì¢…ë£Œ: ìµœëŒ€ ìŠ¤í… {MAX_STEPS} ë„ë‹¬")

env.close()
